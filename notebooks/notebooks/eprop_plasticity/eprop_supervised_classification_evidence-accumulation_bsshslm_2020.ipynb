{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9860bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# eprop_supervised_classification_evidence-accumulation_bsshslm_2020.py\n",
    "#\n",
    "# This file is part of NEST.\n",
    "#\n",
    "# Copyright (C) 2004 The NEST Initiative\n",
    "#\n",
    "# NEST is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 2 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# NEST is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with NEST.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "r\"\"\"\n",
    "Tutorial on learning to accumulate evidence with e-prop after Bellec et al. (2020)\n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "Training a classification model using supervised e-prop plasticity to accumulate evidence.\n",
    "\n",
    "Description\n",
    "~~~~~~~~~~~\n",
    "\n",
    "This script demonstrates supervised learning of a classification task with the eligibility propagation (e-prop)\n",
    "plasticity mechanism by Bellec et al. [1]_.\n",
    "\n",
    "This type of learning is demonstrated at the proof-of-concept task in [1]_. We based this script on their\n",
    "TensorFlow script given in [2]_.\n",
    "\n",
    "The task, a so-called evidence accumulation task, is inspired by behavioral tasks, where a lab animal (e.g., a\n",
    "mouse) runs along a track, gets cues on the left and right, and has to decide at the end of the track between\n",
    "taking a left and a right turn of which one is correct. After a number of iterations, the animal is able to\n",
    "infer the underlying rationale of the task. Here, the solution is to turn to the side in which more cues were\n",
    "presented.\n",
    "\n",
    ".. image:: eprop_supervised_classification_evidence-accumulation_bsshslm_2020.png\n",
    "   :width: 70 %\n",
    "   :alt: Schematic of network architecture. Same as Figure 1 in the code.\n",
    "   :align: center\n",
    "\n",
    "Learning in the neural network model is achieved by optimizing the connection weights with e-prop plasticity.\n",
    "This plasticity rule requires a specific network architecture depicted in Figure 1. The neural network model\n",
    "consists of a recurrent network that receives input from spike generators and projects onto two readout\n",
    "neurons - one for the left and one for the right turn at the end. The input neuron population consists of four\n",
    "groups: one group providing background noise of a specific rate for some base activity throughout the\n",
    "experiment, one group providing the input spikes of the left cues and one group providing them for the right\n",
    "cues, and a last group defining the recall window, in which the network has to decide. The readout neuron\n",
    "compares the network signal :math:`\\pi_k` with the teacher target signal :math:`\\pi_k^*`, which it receives from\n",
    "a rate generator. Since the decision is at the end and all the cues are relevant, the network has to keep the\n",
    "cues in memory. Additional adaptive neurons in the network enable this memory. The network's training error is\n",
    "assessed by employing a cross-entropy error loss.\n",
    "\n",
    "Details on the event-based NEST implementation of e-prop can be found in [3]_.\n",
    "\n",
    "References\n",
    "~~~~~~~~~~\n",
    "\n",
    ".. [1] Bellec G, Scherr F, Subramoney F, Hajek E, Salaj D, Legenstein R, Maass W (2020). A solution to the\n",
    "       learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11:3625.\n",
    "       https://doi.org/10.1038/s41467-020-17236-y\n",
    "\n",
    ".. [2] https://github.com/IGITUGraz/eligibility_propagation/blob/master/Figure_3_and_S7_e_prop_tutorials/tutorial_evidence_accumulation_with_alif.py\n",
    "\n",
    ".. [3] Korcsak-Gorzo A, Stapmanns J, Espinoza Valverde JA, Plesser HE,\n",
    "       Dahmen D, Bolten M, Van Albada SJ, Diesmann M. Event-based\n",
    "       implementation of eligibility propagation (in preparation)\n",
    "\n",
    "\"\"\"  # pylint: disable=line-too-long # noqa: E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834cea6",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "# We begin by importing all libraries required for the simulation, analysis, and visualization.\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nest\n",
    "import numpy as np\n",
    "from cycler import cycler\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12056a37",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Schematic of network architecture\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# This figure, identical to the one in the description, shows the required network architecture in the center,\n",
    "# the input and output of the classification task above, and lists of the required NEST device, neuron, and\n",
    "# synapse models below. The connections that must be established are numbered 1 to 7.\n",
    "\n",
    "try:\n",
    "    Image(filename=\"./eprop_supervised_classification_evidence-accumulation_bsshslm_2020.png\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de69b4",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "# ~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12f506",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Initialize random generator\n",
    "# ...........................\n",
    "# We seed the numpy random generator, which will generate random initial weights as well as random input and\n",
    "# output.\n",
    "\n",
    "rng_seed = 1  # numpy random seed\n",
    "np.random.seed(rng_seed)  # fix numpy random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925b607",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Define timing of task\n",
    "# .....................\n",
    "# The task's temporal structure is then defined, once as time steps and once as durations in milliseconds.\n",
    "# Using a batch size larger than one aids the network in generalization, facilitating the solution to this task.\n",
    "# The original number of iterations requires distributed computing. Increasing the number of iterations\n",
    "# enhances learning performance up to the point where overfitting occurs. If early stopping is enabled, the\n",
    "# classification error is tested in regular intervals and the training stopped as soon as the error selected as\n",
    "# stop criterion is reached. After training, the performance can be tested over a number of test iterations.\n",
    "\n",
    "batch_size = 32  # batch size, 64 in reference [2], 32 in the README to reference [2]\n",
    "n_iter_train = 50  # number of training iterations, 2000 in reference [2]\n",
    "n_iter_test = 4  # number of iterations for final test\n",
    "do_early_stopping = True  # if True, stop training as soon as stop criterion fulfilled\n",
    "n_iter_validate_every = 10  # number of training iterations before validation\n",
    "n_iter_early_stop = 8  # number of iterations to average over to evaluate early stopping condition\n",
    "stop_crit = 0.07  # error value corresponding to stop criterion for early stopping\n",
    "\n",
    "input = {\n",
    "    \"n_symbols\": 4,  # number of input populations, e.g. 4 = left, right, recall, noise\n",
    "    \"n_cues\": 7,  # number of cues given before decision\n",
    "    \"prob_group\": 0.3,  # probability with which one input group is present\n",
    "    \"spike_prob\": 0.04,  # spike probability of frozen input noise\n",
    "}\n",
    "\n",
    "steps = {\n",
    "    \"cue\": 100,  # time steps in one cue presentation\n",
    "    \"spacing\": 50,  # time steps of break between two cues\n",
    "    \"bg_noise\": 1050,  # time steps of background noise\n",
    "    \"recall\": 150,  # time steps of recall\n",
    "}\n",
    "\n",
    "steps[\"cues\"] = input[\"n_cues\"] * (steps[\"cue\"] + steps[\"spacing\"])  # time steps of all cues\n",
    "steps[\"sequence\"] = steps[\"cues\"] + steps[\"bg_noise\"] + steps[\"recall\"]  # time steps of one full sequence\n",
    "steps[\"learning_window\"] = steps[\"recall\"]  # time steps of window with non-zero learning signals\n",
    "\n",
    "steps.update(\n",
    "    {\n",
    "        \"offset_gen\": 1,  # offset since generator signals start from time step 1\n",
    "        \"delay_in_rec\": 1,  # connection delay between input and recurrent neurons\n",
    "        \"delay_rec_out\": 1,  # connection delay between recurrent and output neurons\n",
    "        \"delay_out_norm\": 1,  # connection delay between output neurons for normalization\n",
    "        \"extension_sim\": 1,  # extra time step to close right-open simulation time interval in Simulate()\n",
    "        \"final_update\": 3,  # extra time steps to update all synapses at the end of task\n",
    "    }\n",
    ")\n",
    "\n",
    "steps[\"delays\"] = steps[\"delay_in_rec\"] + steps[\"delay_rec_out\"] + steps[\"delay_out_norm\"]  # time steps of delays\n",
    "\n",
    "steps[\"total_offset\"] = steps[\"offset_gen\"] + steps[\"delays\"]  # time steps of total offset\n",
    "\n",
    "duration = {\"step\": 1.0}  # ms, temporal resolution of the simulation\n",
    "\n",
    "duration.update({key: value * duration[\"step\"] for key, value in steps.items()})  # ms, durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e42fa",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Set up simulation\n",
    "# .................\n",
    "# As last step of the setup, we reset the NEST kernel to remove all existing NEST simulation settings and\n",
    "# objects and set some NEST kernel parameters, some of which are e-prop-related.\n",
    "\n",
    "params_setup = {\n",
    "    \"eprop_learning_window\": duration[\"learning_window\"],\n",
    "    \"eprop_reset_neurons_on_update\": True,  # if True, reset dynamic variables at start of each update interval\n",
    "    \"eprop_update_interval\": duration[\"sequence\"],  # ms, time interval for updating the synaptic weights\n",
    "    \"print_time\": False,  # if True, print time progress bar during simulation, set False if run as code cell\n",
    "    \"resolution\": duration[\"step\"],\n",
    "    \"total_num_virtual_procs\": 1,  # number of virtual processes, set in case of distributed computing\n",
    "}\n",
    "\n",
    "####################\n",
    "\n",
    "nest.ResetKernel()\n",
    "nest.set(**params_setup)\n",
    "nest.set_verbosity(\"M_FATAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315d98b",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Create neurons\n",
    "# ~~~~~~~~~~~~~~\n",
    "# We proceed by creating a certain number of input, recurrent, and readout neurons and setting their parameters.\n",
    "# Additionally, we already create an input spike generator and an output target rate generator, which we will\n",
    "# configure later. Within the recurrent network, alongside a population of regular neurons, we introduce a\n",
    "# population of adaptive neurons, to enhance the network's memory retention.\n",
    "\n",
    "n_in = 40  # number of input neurons\n",
    "n_ad = 50  # number of adaptive neurons\n",
    "n_reg = 50  # number of regular neurons\n",
    "n_rec = n_ad + n_reg  # number of recurrent neurons\n",
    "n_out = 2  # number of readout neurons\n",
    "\n",
    "params_nrn_out = {\n",
    "    \"C_m\": 1.0,  # pF, membrane capacitance - takes effect only if neurons get current input (here not the case)\n",
    "    \"E_L\": 0.0,  # mV, leak / resting membrane potential\n",
    "    \"I_e\": 0.0,  # pA, external current input\n",
    "    \"loss\": \"cross_entropy\",  # loss function\n",
    "    \"regular_spike_arrival\": False,  # If True, input spikes arrive at end of time step, if False at beginning\n",
    "    \"tau_m\": 20.0,  # ms, membrane time constant\n",
    "    \"V_m\": 0.0,  # mV, initial value of the membrane voltage\n",
    "}\n",
    "\n",
    "params_nrn_reg = {\n",
    "    \"beta\": 1.0,  # width scaling of the pseudo-derivative\n",
    "    \"C_m\": 1.0,\n",
    "    \"c_reg\": 300.0,  # coefficient of firing rate regularization - 2*learning_window*(TF c_reg) for technical reasons\n",
    "    \"E_L\": 0.0,\n",
    "    \"f_target\": 10.0,  # spikes/s, target firing rate for firing rate regularization\n",
    "    \"gamma\": 0.3,  # height scaling of the pseudo-derivative\n",
    "    \"I_e\": 0.0,\n",
    "    \"regular_spike_arrival\": True,\n",
    "    \"surrogate_gradient_function\": \"piecewise_linear\",  # surrogate gradient / pseudo-derivative function\n",
    "    \"t_ref\": 5.0,  # ms, duration of refractory period\n",
    "    \"tau_m\": 20.0,\n",
    "    \"V_m\": 0.0,\n",
    "    \"V_th\": 0.6,  # mV, spike threshold membrane voltage\n",
    "}\n",
    "\n",
    "# factors from the original pseudo-derivative definition are incorporated into the parameters\n",
    "params_nrn_reg[\"gamma\"] /= params_nrn_reg[\"V_th\"]\n",
    "params_nrn_reg[\"beta\"] /= np.abs(params_nrn_reg[\"V_th\"])  # prefactor is inside abs in the original definition\n",
    "\n",
    "params_nrn_ad = {\n",
    "    \"beta\": 1.0,\n",
    "    \"adapt_tau\": 2000.0,  # ms, time constant of adaptive threshold\n",
    "    \"adaptation\": 0.0,  # initial value of the spike threshold adaptation\n",
    "    \"C_m\": 1.0,\n",
    "    \"c_reg\": 300.0,\n",
    "    \"E_L\": 0.0,\n",
    "    \"f_target\": 10.0,\n",
    "    \"gamma\": 0.3,\n",
    "    \"I_e\": 0.0,\n",
    "    \"regular_spike_arrival\": True,\n",
    "    \"surrogate_gradient_function\": \"piecewise_linear\",\n",
    "    \"t_ref\": 5.0,\n",
    "    \"tau_m\": 20.0,\n",
    "    \"V_m\": 0.0,\n",
    "    \"V_th\": 0.6,\n",
    "}\n",
    "\n",
    "params_nrn_ad[\"gamma\"] /= params_nrn_ad[\"V_th\"]\n",
    "params_nrn_ad[\"beta\"] /= np.abs(params_nrn_ad[\"V_th\"])\n",
    "\n",
    "params_nrn_ad[\"adapt_beta\"] = 1.7 * (\n",
    "    (1.0 - np.exp(-duration[\"step\"] / params_nrn_ad[\"adapt_tau\"]))\n",
    "    / (1.0 - np.exp(-duration[\"step\"] / params_nrn_ad[\"tau_m\"]))\n",
    ")  # prefactor of adaptive threshold\n",
    "\n",
    "####################\n",
    "\n",
    "# Intermediate parrot neurons required between input spike generators and recurrent neurons,\n",
    "# since devices cannot establish plastic synapses for technical reasons\n",
    "\n",
    "gen_spk_in = nest.Create(\"spike_generator\", n_in)\n",
    "nrns_in = nest.Create(\"parrot_neuron\", n_in)\n",
    "\n",
    "# The suffix _bsshslm_2020 follows the NEST convention to indicate in the model name the paper\n",
    "# that introduced it by the first letter of the authors' last names and the publication year.\n",
    "\n",
    "nrns_reg = nest.Create(\"eprop_iaf_bsshslm_2020\", n_reg, params_nrn_reg)\n",
    "nrns_ad = nest.Create(\"eprop_iaf_adapt_bsshslm_2020\", n_ad, params_nrn_ad)\n",
    "nrns_out = nest.Create(\"eprop_readout_bsshslm_2020\", n_out, params_nrn_out)\n",
    "gen_rate_target = nest.Create(\"step_rate_generator\", n_out)\n",
    "\n",
    "nrns_rec = nrns_reg + nrns_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4895459",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Create recorders\n",
    "# ~~~~~~~~~~~~~~~~\n",
    "# We also create recorders, which, while not required for the training, will allow us to track various dynamic\n",
    "# variables of the neurons, spikes, and changes in synaptic weights. To save computing time and memory, the\n",
    "# recorders, the recorded variables, neurons, and synapses can be limited to the ones relevant to the\n",
    "# experiment, and the recording interval can be increased (see the documentation on the specific recorders). By\n",
    "# default, recordings are stored in memory but can also be written to file.\n",
    "\n",
    "n_record = 1  # number of neurons per type to record dynamic variables from - this script requires n_record >= 1\n",
    "n_record_w = 5  # number of senders and targets to record weights from - this script requires n_record_w >=1\n",
    "\n",
    "if n_record == 0 or n_record_w == 0:\n",
    "    raise ValueError(\"n_record and n_record_w >= 1 required\")\n",
    "\n",
    "params_mm_reg = {\n",
    "    \"interval\": duration[\"step\"],  # interval between two recorded time points\n",
    "    \"record_from\": [\"V_m\", \"surrogate_gradient\", \"learning_signal\"],  # dynamic variables to record\n",
    "    \"start\": duration[\"offset_gen\"] + duration[\"delay_in_rec\"],  # start time of recording\n",
    "    \"label\": \"multimeter_reg\",\n",
    "}\n",
    "\n",
    "params_mm_ad = {\n",
    "    \"interval\": duration[\"step\"],\n",
    "    \"record_from\": params_mm_reg[\"record_from\"] + [\"V_th_adapt\", \"adaptation\"],\n",
    "    \"start\": duration[\"offset_gen\"] + duration[\"delay_in_rec\"],\n",
    "    \"label\": \"multimeter_ad\",\n",
    "}\n",
    "\n",
    "params_mm_out = {\n",
    "    \"interval\": duration[\"step\"],\n",
    "    \"record_from\": [\"V_m\", \"readout_signal\", \"readout_signal_unnorm\", \"target_signal\", \"error_signal\"],\n",
    "    \"start\": duration[\"total_offset\"],\n",
    "    \"label\": \"multimeter_out\",\n",
    "}\n",
    "\n",
    "params_wr = {\n",
    "    \"senders\": nrns_in[:n_record_w] + nrns_rec[:n_record_w],  # limit senders to subsample weights to record\n",
    "    \"targets\": nrns_rec[:n_record_w] + nrns_out,  # limit targets to subsample weights to record from\n",
    "    \"start\": duration[\"total_offset\"],\n",
    "    \"label\": \"weight_recorder\",\n",
    "}\n",
    "\n",
    "params_sr_in = {\n",
    "    \"start\": duration[\"offset_gen\"],\n",
    "    \"label\": \"spike_recorder_in\",\n",
    "}\n",
    "\n",
    "params_sr_reg = {\n",
    "    \"start\": duration[\"offset_gen\"],\n",
    "    \"label\": \"spike_recorder_reg\",\n",
    "}\n",
    "\n",
    "params_sr_ad = {\n",
    "    \"start\": duration[\"offset_gen\"],\n",
    "    \"label\": \"spike_recorder_ad\",\n",
    "}\n",
    "\n",
    "####################\n",
    "\n",
    "mm_reg = nest.Create(\"multimeter\", params_mm_reg)\n",
    "mm_ad = nest.Create(\"multimeter\", params_mm_ad)\n",
    "mm_out = nest.Create(\"multimeter\", params_mm_out)\n",
    "sr_in = nest.Create(\"spike_recorder\", params_sr_in)\n",
    "sr_reg = nest.Create(\"spike_recorder\", params_sr_reg)\n",
    "sr_ad = nest.Create(\"spike_recorder\", params_sr_ad)\n",
    "wr = nest.Create(\"weight_recorder\", params_wr)\n",
    "\n",
    "nrns_reg_record = nrns_reg[:n_record]\n",
    "nrns_ad_record = nrns_ad[:n_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aebe5d",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Create connections\n",
    "# ~~~~~~~~~~~~~~~~~~\n",
    "# Now, we define the connectivity and set up the synaptic parameters, with the synaptic weights drawn from\n",
    "# normal distributions. After these preparations, we establish the enumerated connections of the core network,\n",
    "# as well as additional connections to the recorders.\n",
    "\n",
    "params_conn_all_to_all = {\"rule\": \"all_to_all\", \"allow_autapses\": False}\n",
    "params_conn_one_to_one = {\"rule\": \"one_to_one\"}\n",
    "\n",
    "\n",
    "def calculate_glorot_dist(fan_in, fan_out):\n",
    "    glorot_scale = 1.0 / max(1.0, (fan_in + fan_out) / 2.0)\n",
    "    glorot_limit = np.sqrt(3.0 * glorot_scale)\n",
    "    glorot_distribution = np.random.uniform(low=-glorot_limit, high=glorot_limit, size=(fan_in, fan_out))\n",
    "    return glorot_distribution\n",
    "\n",
    "\n",
    "dtype_weights = np.float32  # data type of weights - for reproducing TF results set to np.float32\n",
    "weights_in_rec = np.array(np.random.randn(n_in, n_rec).T / np.sqrt(n_in), dtype=dtype_weights)\n",
    "weights_rec_rec = np.array(np.random.randn(n_rec, n_rec).T / np.sqrt(n_rec), dtype=dtype_weights)\n",
    "np.fill_diagonal(weights_rec_rec, 0.0)  # since no autapses set corresponding weights to zero\n",
    "weights_rec_out = np.array(calculate_glorot_dist(n_rec, n_out).T, dtype=dtype_weights)\n",
    "weights_out_rec = np.array(np.random.randn(n_rec, n_out), dtype=dtype_weights)\n",
    "\n",
    "params_common_syn_eprop = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adam\",  # algorithm to optimize the weights\n",
    "        \"batch_size\": batch_size,\n",
    "        \"beta_1\": 0.9,  # exponential decay rate for 1st moment estimate of Adam optimizer\n",
    "        \"beta_2\": 0.999,  # exponential decay rate for 2nd moment raw estimate of Adam optimizer\n",
    "        \"epsilon\": 1e-8,  # small numerical stabilization constant of Adam optimizer\n",
    "        \"Wmin\": -100.0,  # pA, minimal limit of the synaptic weights\n",
    "        \"Wmax\": 100.0,  # pA, maximal limit of the synaptic weights\n",
    "    },\n",
    "    \"average_gradient\": True,  # if True, average the gradient over the learning window\n",
    "    \"weight_recorder\": wr,\n",
    "}\n",
    "\n",
    "eta_test = 0.0  # learning rate for test phase\n",
    "eta_train = 5e-3  # learning rate for training phase\n",
    "\n",
    "params_syn_base = {\n",
    "    \"synapse_model\": \"eprop_synapse_bsshslm_2020\",\n",
    "    \"delay\": duration[\"step\"],  # ms, dendritic delay\n",
    "    \"tau_m_readout\": params_nrn_out[\"tau_m\"],  # ms, for technical reasons pass readout neuron membrane time constant\n",
    "}\n",
    "\n",
    "params_syn_in = params_syn_base.copy()\n",
    "params_syn_in[\"weight\"] = weights_in_rec  # pA, initial values for the synaptic weights\n",
    "\n",
    "params_syn_rec = params_syn_base.copy()\n",
    "params_syn_rec[\"weight\"] = weights_rec_rec\n",
    "\n",
    "params_syn_out = params_syn_base.copy()\n",
    "params_syn_out[\"weight\"] = weights_rec_out\n",
    "\n",
    "params_syn_feedback = {\n",
    "    \"synapse_model\": \"eprop_learning_signal_connection_bsshslm_2020\",\n",
    "    \"delay\": duration[\"step\"],\n",
    "    \"weight\": weights_out_rec,\n",
    "}\n",
    "\n",
    "params_syn_out_out = {\n",
    "    \"synapse_model\": \"rate_connection_delayed\",\n",
    "    \"delay\": duration[\"step\"],\n",
    "    \"receptor_type\": 1,  # receptor type of readout neuron to receive other readout neuron's signals for softmax\n",
    "    \"weight\": 1.0,  # pA, weight 1.0 required for correct softmax computation for technical reasons\n",
    "}\n",
    "\n",
    "params_syn_rate_target = {\n",
    "    \"synapse_model\": \"rate_connection_delayed\",\n",
    "    \"delay\": duration[\"step\"],\n",
    "    \"receptor_type\": 2,  # receptor type over which readout neuron receives target signal\n",
    "}\n",
    "\n",
    "params_syn_static = {\n",
    "    \"synapse_model\": \"static_synapse\",\n",
    "    \"delay\": duration[\"step\"],\n",
    "}\n",
    "\n",
    "params_init_optimizer = {\n",
    "    \"optimizer\": {\n",
    "        \"m\": 0.0,  # initial 1st moment estimate m of Adam optimizer\n",
    "        \"v\": 0.0,  # initial 2nd moment raw estimate v of Adam optimizer\n",
    "    }\n",
    "}\n",
    "\n",
    "####################\n",
    "\n",
    "nest.SetDefaults(\"eprop_synapse_bsshslm_2020\", params_common_syn_eprop)\n",
    "\n",
    "nest.Connect(gen_spk_in, nrns_in, params_conn_one_to_one, params_syn_static)  # connection 1\n",
    "nest.Connect(nrns_in, nrns_rec, params_conn_all_to_all, params_syn_in)  # connection 2\n",
    "nest.Connect(nrns_rec, nrns_rec, params_conn_all_to_all, params_syn_rec)  # connection 3\n",
    "nest.Connect(nrns_rec, nrns_out, params_conn_all_to_all, params_syn_out)  # connection 4\n",
    "nest.Connect(nrns_out, nrns_rec, params_conn_all_to_all, params_syn_feedback)  # connection 5\n",
    "nest.Connect(gen_rate_target, nrns_out, params_conn_one_to_one, params_syn_rate_target)  # connection 6\n",
    "nest.Connect(nrns_out, nrns_out, params_conn_all_to_all, params_syn_out_out)  # connection 7\n",
    "\n",
    "nest.Connect(nrns_in, sr_in, params_conn_all_to_all, params_syn_static)\n",
    "nest.Connect(nrns_reg, sr_reg, params_conn_all_to_all, params_syn_static)\n",
    "nest.Connect(nrns_ad, sr_ad, params_conn_all_to_all, params_syn_static)\n",
    "\n",
    "nest.Connect(mm_reg, nrns_reg_record, params_conn_all_to_all, params_syn_static)\n",
    "nest.Connect(mm_ad, nrns_ad_record, params_conn_all_to_all, params_syn_static)\n",
    "nest.Connect(mm_out, nrns_out, params_conn_all_to_all, params_syn_static)\n",
    "\n",
    "# After creating the connections, we can individually initialize the optimizer's\n",
    "# dynamic variables for single synapses (here exemplarily for two connections).\n",
    "\n",
    "nest.GetConnections(nrns_rec[0], nrns_rec[1:3]).set([params_init_optimizer] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c209a0",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Create input and output\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# We generate the input as four neuron populations, two producing the left and right cues, respectively, one the\n",
    "# recall signal and one the background input throughout the task. The sequence of cues is drawn with a\n",
    "# probability that favors one side. For each such sequence, the favored side, the solution or target, is\n",
    "# assigned randomly to the left or right.\n",
    "\n",
    "\n",
    "def generate_evidence_accumulation_input_output(batch_size, n_in, steps, input):\n",
    "    n_pop_nrn = n_in // input[\"n_symbols\"]\n",
    "\n",
    "    prob_choices = np.array([input[\"prob_group\"], 1 - input[\"prob_group\"]], dtype=np.float32)\n",
    "    idx = np.random.choice([0, 1], batch_size)\n",
    "    probs = np.zeros((batch_size, 2), dtype=np.float32)\n",
    "    probs[:, 0] = prob_choices[idx]\n",
    "    probs[:, 1] = prob_choices[1 - idx]\n",
    "\n",
    "    batched_cues = np.zeros((batch_size, input[\"n_cues\"]), dtype=int)\n",
    "    for b_idx in range(batch_size):\n",
    "        batched_cues[b_idx, :] = np.random.choice([0, 1], input[\"n_cues\"], p=probs[b_idx])\n",
    "\n",
    "    input_spike_probs = np.zeros((batch_size, steps[\"sequence\"], n_in))\n",
    "\n",
    "    for b_idx in range(batch_size):\n",
    "        for c_idx in range(input[\"n_cues\"]):\n",
    "            cue = batched_cues[b_idx, c_idx]\n",
    "\n",
    "            step_start = c_idx * (steps[\"cue\"] + steps[\"spacing\"]) + steps[\"spacing\"]\n",
    "            step_stop = step_start + steps[\"cue\"]\n",
    "\n",
    "            pop_nrn_start = cue * n_pop_nrn\n",
    "            pop_nrn_stop = pop_nrn_start + n_pop_nrn\n",
    "\n",
    "            input_spike_probs[b_idx, step_start:step_stop, pop_nrn_start:pop_nrn_stop] = input[\"spike_prob\"]\n",
    "\n",
    "    input_spike_probs[:, -steps[\"recall\"] :, 2 * n_pop_nrn : 3 * n_pop_nrn] = input[\"spike_prob\"]\n",
    "    input_spike_probs[:, :, 3 * n_pop_nrn :] = input[\"spike_prob\"] / 4.0\n",
    "    input_spike_bools = input_spike_probs > np.random.rand(input_spike_probs.size).reshape(input_spike_probs.shape)\n",
    "    input_spike_bools[:, 0, :] = 0  # remove spikes in 0th time step of every sequence for technical reasons\n",
    "\n",
    "    target_cues = np.zeros(batch_size, dtype=int)\n",
    "    target_cues[:] = np.sum(batched_cues, axis=1) > int(input[\"n_cues\"] / 2)\n",
    "\n",
    "    return input_spike_bools, target_cues\n",
    "\n",
    "\n",
    "def get_params_task_input_output(n_iter_interval):\n",
    "    iteration_offset = n_iter_interval * batch_size * duration[\"sequence\"]\n",
    "    dtype_in_spks = np.float32  # data type of input spikes - for reproducing TF results set to np.float32\n",
    "\n",
    "    input_spike_bools, target_cues = generate_evidence_accumulation_input_output(batch_size, n_in, steps, input)\n",
    "\n",
    "    input_spike_bools_arr = np.array(input_spike_bools).reshape(batch_size * steps[\"sequence\"], n_in)\n",
    "    timeline_task = (\n",
    "        np.arange(0.0, batch_size * duration[\"sequence\"], duration[\"step\"]) + iteration_offset + duration[\"offset_gen\"]\n",
    "    )\n",
    "\n",
    "    params_gen_spk_in = [\n",
    "        {\"spike_times\": timeline_task[input_spike_bools_arr[:, nrn_in_idx]].astype(dtype_in_spks)}\n",
    "        for nrn_in_idx in range(n_in)\n",
    "    ]\n",
    "\n",
    "    target_rate_changes = np.zeros((n_out, batch_size))\n",
    "    target_rate_changes[np.array(target_cues), np.arange(batch_size)] = 1\n",
    "\n",
    "    params_gen_rate_target = [\n",
    "        {\n",
    "            \"amplitude_times\": np.arange(0.0, batch_size * duration[\"sequence\"], duration[\"sequence\"])\n",
    "            + iteration_offset\n",
    "            + duration[\"total_offset\"],\n",
    "            \"amplitude_values\": target_rate_changes[nrn_out_idx],\n",
    "        }\n",
    "        for nrn_out_idx in range(n_out)\n",
    "    ]\n",
    "\n",
    "    return params_gen_spk_in, params_gen_rate_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8615e",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Force final update\n",
    "# ~~~~~~~~~~~~~~~~~~\n",
    "# Synapses only get active, that is, the correct weight update calculated and applied, when they transmit a\n",
    "# spike. To still be able to read out the correct weights at the end of the simulation, we force spiking of the\n",
    "# presynaptic neuron and thus an update of all synapses, including those that have not transmitted a spike in\n",
    "# the last update interval, by sending a strong spike to all neurons that form the presynaptic side of an eprop\n",
    "# synapse. This step is required purely for technical reasons.\n",
    "\n",
    "gen_spk_final_update = nest.Create(\"spike_generator\", 1)\n",
    "\n",
    "nest.Connect(gen_spk_final_update, nrns_in + nrns_rec, \"all_to_all\", {\"weight\": 1000.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03669ac",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Read out pre-training weights\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Before we begin training, we read out the initial weight matrices so that we can eventually compare them to\n",
    "# the optimized weights.\n",
    "\n",
    "\n",
    "def get_weights(pop_pre, pop_post):\n",
    "    conns = nest.GetConnections(pop_pre, pop_post).get([\"source\", \"target\", \"weight\"])\n",
    "    conns[\"senders\"] = np.array(conns[\"source\"]) - np.min(conns[\"source\"])\n",
    "    conns[\"targets\"] = np.array(conns[\"target\"]) - np.min(conns[\"target\"])\n",
    "\n",
    "    conns[\"weight_matrix\"] = np.zeros((len(pop_post), len(pop_pre)))\n",
    "    conns[\"weight_matrix\"][conns[\"targets\"], conns[\"senders\"]] = conns[\"weight\"]\n",
    "    return conns\n",
    "\n",
    "\n",
    "weights_pre_train = {\n",
    "    \"in_rec\": get_weights(nrns_in, nrns_rec),\n",
    "    \"rec_rec\": get_weights(nrns_rec, nrns_rec),\n",
    "    \"rec_out\": get_weights(nrns_rec, nrns_out),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a68c0",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Simulate and evaluate\n",
    "# ~~~~~~~~~~~~~~~~~~~~~\n",
    "# We train the network by simulating for a number of training iterations with the set learning rate. If early\n",
    "# stopping is turned on, we evaluate the network's performance on the validation set in regular intervals and,\n",
    "# if the error is below a certain threshold, we stop the training early. If the error is not below the\n",
    "# threshold, we continue training until the end of the set number of iterations. Finally, we evaluate the\n",
    "# network's performance on the test set.\n",
    "# Furthermore, we evaluate the network's training error by calculating a loss - in this case, the cross-entropy\n",
    "# error between the integrated recurrent network activity and the target rate.\n",
    "\n",
    "\n",
    "class TrainingPipeline:\n",
    "    def __init__(self):\n",
    "        self.results_dict = {\n",
    "            \"error\": [],\n",
    "            \"loss\": [],\n",
    "            \"iteration\": [],\n",
    "            \"label\": [],\n",
    "        }\n",
    "        self.n_iter_sim = 0\n",
    "        self.phase_label_previous = \"\"\n",
    "        self.error = 0\n",
    "        self.k_iter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def evaluate(self):\n",
    "        events_mm_out = mm_out.get(\"events\")\n",
    "\n",
    "        readout_signal = events_mm_out[\"readout_signal\"]  # corresponds to softmax\n",
    "        target_signal = events_mm_out[\"target_signal\"]\n",
    "        senders = events_mm_out[\"senders\"]\n",
    "        times = events_mm_out[\"times\"]\n",
    "\n",
    "        cond1 = times > (self.n_iter_sim - 1) * batch_size * duration[\"sequence\"] + duration[\"total_offset\"]\n",
    "        cond2 = times <= self.n_iter_sim * batch_size * duration[\"sequence\"] + duration[\"total_offset\"]\n",
    "        idc = cond1 & cond2\n",
    "\n",
    "        readout_signal = np.array([readout_signal[idc][senders[idc] == i] for i in set(senders)])\n",
    "        target_signal = np.array([target_signal[idc][senders[idc] == i] for i in set(senders)])\n",
    "\n",
    "        readout_signal = readout_signal.reshape((n_out, 1, batch_size, steps[\"sequence\"]))\n",
    "        target_signal = target_signal.reshape((n_out, 1, batch_size, steps[\"sequence\"]))\n",
    "\n",
    "        readout_signal = readout_signal[:, :, :, -steps[\"learning_window\"] :]\n",
    "        target_signal = target_signal[:, :, :, -steps[\"learning_window\"] :]\n",
    "\n",
    "        loss = -np.mean(np.sum(target_signal * np.log(readout_signal), axis=0), axis=(1, 2))\n",
    "\n",
    "        y_prediction = np.argmax(np.mean(readout_signal, axis=3), axis=0)\n",
    "        y_target = np.argmax(np.mean(target_signal, axis=3), axis=0)\n",
    "        accuracy = np.mean((y_target == y_prediction), axis=1)\n",
    "        errors = 1.0 - accuracy\n",
    "\n",
    "        self.results_dict[\"iteration\"].append(self.n_iter_sim)\n",
    "        self.results_dict[\"error\"].extend(errors)\n",
    "        self.results_dict[\"loss\"].extend(loss)\n",
    "        self.results_dict[\"label\"].append(self.phase_label_previous)\n",
    "\n",
    "        self.error = errors[0]\n",
    "\n",
    "    def run_phase(self, phase_label, eta):\n",
    "        params_common_syn_eprop[\"optimizer\"][\"eta\"] = eta\n",
    "        nest.SetDefaults(\"eprop_synapse_bsshslm_2020\", params_common_syn_eprop)\n",
    "\n",
    "        params_gen_spk_in, params_gen_rate_target = get_params_task_input_output(self.n_iter_sim)\n",
    "        nest.SetStatus(gen_spk_in, params_gen_spk_in)\n",
    "        nest.SetStatus(gen_rate_target, params_gen_rate_target)\n",
    "\n",
    "        self.simulate(\"total_offset\")\n",
    "        self.simulate(\"extension_sim\")\n",
    "\n",
    "        if self.n_iter_sim > 0:\n",
    "            self.evaluate()\n",
    "\n",
    "        duration[\"sim\"] = batch_size * duration[\"sequence\"] - duration[\"total_offset\"] - duration[\"extension_sim\"]\n",
    "\n",
    "        self.simulate(\"sim\")\n",
    "\n",
    "        self.n_iter_sim += 1\n",
    "        self.phase_label_previous = phase_label\n",
    "\n",
    "    def run_training(self):\n",
    "        self.run_phase(\"training\", eta_train)\n",
    "\n",
    "    def run_validation(self):\n",
    "        if do_early_stopping and self.k_iter % n_iter_validate_every == 0:\n",
    "            self.run_phase(\"validation\", eta_test)\n",
    "\n",
    "    def run_early_stopping(self):\n",
    "        if do_early_stopping and self.k_iter % n_iter_validate_every == 0:\n",
    "            if self.k_iter > 0 and self.error < stop_crit:\n",
    "                errors_early_stop = []\n",
    "                for _ in range(n_iter_early_stop):\n",
    "                    self.run_phase(\"early-stopping\", eta_test)\n",
    "                    errors_early_stop.append(self.error)\n",
    "\n",
    "                self.early_stop = np.mean(errors_early_stop) < stop_crit\n",
    "\n",
    "    def run_test(self):\n",
    "        for _ in range(n_iter_test):\n",
    "            self.run_phase(\"test\", eta_test)\n",
    "\n",
    "    def simulate(self, k):\n",
    "        nest.Simulate(duration[k])\n",
    "\n",
    "    def run(self):\n",
    "        while self.k_iter < n_iter_train and not self.early_stop:\n",
    "            self.run_validation()\n",
    "            self.run_early_stopping()\n",
    "            self.run_training()\n",
    "            self.k_iter += 1\n",
    "\n",
    "        self.run_test()\n",
    "\n",
    "        self.simulate(\"total_offset\")\n",
    "        self.simulate(\"extension_sim\")\n",
    "\n",
    "        self.evaluate()\n",
    "\n",
    "        duration[\"task\"] = self.n_iter_sim * batch_size * duration[\"sequence\"] + duration[\"total_offset\"]\n",
    "\n",
    "        gen_spk_final_update.set({\"spike_times\": [duration[\"task\"] + duration[\"extension_sim\"] + 1]})\n",
    "\n",
    "        self.simulate(\"final_update\")\n",
    "\n",
    "    def get_results(self):\n",
    "        for k, v in self.results_dict.items():\n",
    "            self.results_dict[k] = np.array(v)\n",
    "        return self.results_dict\n",
    "\n",
    "\n",
    "training_pipeline = TrainingPipeline()\n",
    "training_pipeline.run()\n",
    "\n",
    "results_dict = training_pipeline.get_results()\n",
    "n_iter_sim = training_pipeline.n_iter_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f08ca",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Read out post-training weights\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# After the training, we can read out the optimized final weights.\n",
    "\n",
    "weights_post_train = {\n",
    "    \"in_rec\": get_weights(nrns_in, nrns_rec),\n",
    "    \"rec_rec\": get_weights(nrns_rec, nrns_rec),\n",
    "    \"rec_out\": get_weights(nrns_rec, nrns_out),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a2cf3",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Read out recorders\n",
    "# ~~~~~~~~~~~~~~~~~~\n",
    "# We can also retrieve the recorded history of the dynamic variables and weights, as well as detected spikes.\n",
    "\n",
    "events_mm_reg = mm_reg.get(\"events\")\n",
    "events_mm_ad = mm_ad.get(\"events\")\n",
    "events_mm_out = mm_out.get(\"events\")\n",
    "events_sr_in = sr_in.get(\"events\")\n",
    "events_sr_reg = sr_reg.get(\"events\")\n",
    "events_sr_ad = sr_ad.get(\"events\")\n",
    "events_wr = wr.get(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35b7e5",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "# ~~~~~~~~~~~~\n",
    "# Then, we plot a series of plots.\n",
    "\n",
    "do_plotting = True  # if True, plot the results\n",
    "\n",
    "if not do_plotting:\n",
    "    exit()\n",
    "\n",
    "colors = {\n",
    "    \"blue\": \"#2854c5ff\",\n",
    "    \"red\": \"#e04b40ff\",\n",
    "    \"green\": \"#25aa2cff\",\n",
    "    \"gold\": \"#f9c643ff\",\n",
    "    \"white\": \"#ffffffff\",\n",
    "}\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.prop_cycle\": cycler(color=[colors[k] for k in [\"blue\", \"red\", \"green\", \"gold\"]]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166a6aa",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Plot learning performance\n",
    "# .........................\n",
    "# We begin with two plots visualizing the learning performance of the network: the loss and the error, both\n",
    "# plotted against the iterations.\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "fig.suptitle(\"Learning performance\")\n",
    "\n",
    "for color, label in zip(colors, set(results_dict[\"label\"])):\n",
    "    idc = results_dict[\"label\"] == label\n",
    "    axs[0].scatter(results_dict[\"iteration\"][idc], results_dict[\"loss\"][idc], label=label)\n",
    "    axs[1].scatter(results_dict[\"iteration\"][idc], results_dict[\"error\"][idc], label=label)\n",
    "\n",
    "axs[0].set_ylabel(r\"$\\mathcal{L} = -\\sum_{t,k} \\pi_k^{*,t} \\log \\pi_k^t$\")\n",
    "axs[1].set_ylabel(\"error\")\n",
    "\n",
    "axs[-1].set_xlabel(\"iteration\")\n",
    "axs[-1].legend(bbox_to_anchor=(1.05, 0.5), loc=\"center left\")\n",
    "axs[-1].xaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b971604",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Plot spikes and dynamic variables\n",
    "# .................................\n",
    "# This plotting routine shows how to plot all of the recorded dynamic variables and spikes across time. We take\n",
    "# one snapshot in the first iteration and one snapshot at the end.\n",
    "\n",
    "\n",
    "def plot_recordable(ax, events, recordable, ylabel, xlims):\n",
    "    for sender in set(events[\"senders\"]):\n",
    "        idc_sender = events[\"senders\"] == sender\n",
    "        idc_times = (events[\"times\"][idc_sender] > xlims[0]) & (events[\"times\"][idc_sender] < xlims[1])\n",
    "        ax.plot(events[\"times\"][idc_sender][idc_times], events[recordable][idc_sender][idc_times], lw=0.5)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    margin = np.abs(np.max(events[recordable]) - np.min(events[recordable])) * 0.1\n",
    "    ax.set_ylim(np.min(events[recordable]) - margin, np.max(events[recordable]) + margin)\n",
    "\n",
    "\n",
    "def plot_spikes(ax, events, ylabel, xlims):\n",
    "    idc_times = (events[\"times\"] > xlims[0]) & (events[\"times\"] < xlims[1])\n",
    "    senders_subset = events[\"senders\"][idc_times]\n",
    "    times_subset = events[\"times\"][idc_times]\n",
    "\n",
    "    ax.scatter(times_subset, senders_subset, s=0.1)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    margin = np.abs(np.max(senders_subset) - np.min(senders_subset)) * 0.1\n",
    "    ax.set_ylim(np.min(senders_subset) - margin, np.max(senders_subset) + margin)\n",
    "\n",
    "\n",
    "for title, xlims in zip(\n",
    "    [\"Dynamic variables before training\", \"Dynamic variables after training\"],\n",
    "    [\n",
    "        (0, steps[\"sequence\"]),\n",
    "        ((n_iter_sim - 1) * batch_size * steps[\"sequence\"], n_iter_sim * batch_size * steps[\"sequence\"]),\n",
    "    ],\n",
    "):\n",
    "    fig, axs = plt.subplots(14, 1, sharex=True, figsize=(8, 14), gridspec_kw={\"hspace\": 0.4, \"left\": 0.2})\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    plot_spikes(axs[0], events_sr_in, r\"$z_i$\" + \"\\n\", xlims)\n",
    "    plot_spikes(axs[1], events_sr_reg, r\"$z_j$\" + \"\\n\", xlims)\n",
    "\n",
    "    plot_recordable(axs[2], events_mm_reg, \"V_m\", r\"$v_j$\" + \"\\n(mV)\", xlims)\n",
    "    plot_recordable(axs[3], events_mm_reg, \"surrogate_gradient\", r\"$\\psi_j$\" + \"\\n\", xlims)\n",
    "    plot_recordable(axs[4], events_mm_reg, \"learning_signal\", r\"$L_j$\" + \"\\n(pA)\", xlims)\n",
    "\n",
    "    plot_spikes(axs[5], events_sr_ad, r\"$z_j$\" + \"\\n\", xlims)\n",
    "\n",
    "    plot_recordable(axs[6], events_mm_ad, \"V_m\", r\"$v_j$\" + \"\\n(mV)\", xlims)\n",
    "    plot_recordable(axs[7], events_mm_ad, \"surrogate_gradient\", r\"$\\psi_j$\" + \"\\n\", xlims)\n",
    "    plot_recordable(axs[8], events_mm_ad, \"V_th_adapt\", r\"$A_j$\" + \"\\n(mV)\", xlims)\n",
    "    plot_recordable(axs[9], events_mm_ad, \"learning_signal\", r\"$L_j$\" + \"\\n(pA)\", xlims)\n",
    "\n",
    "    plot_recordable(axs[10], events_mm_out, \"V_m\", r\"$v_k$\" + \"\\n(mV)\", xlims)\n",
    "    plot_recordable(axs[11], events_mm_out, \"target_signal\", r\"$\\pi^*_k$\" + \"\\n\", xlims)\n",
    "    plot_recordable(axs[12], events_mm_out, \"readout_signal\", r\"$\\pi_k$\" + \"\\n\", xlims)\n",
    "    plot_recordable(axs[13], events_mm_out, \"error_signal\", r\"$\\pi_k-\\pi^*_k$\" + \"\\n\", xlims)\n",
    "\n",
    "    axs[-1].set_xlabel(r\"$t$ (ms)\")\n",
    "    axs[-1].set_xlim(*xlims)\n",
    "\n",
    "    fig.align_ylabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509d45e",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Plot weight time courses\n",
    "# ........................\n",
    "# Similarly, we can plot the weight histories. Note that the weight recorder, attached to the synapses, works\n",
    "# differently than the other recorders. Since synapses only get activated when they transmit a spike, the weight\n",
    "# recorder only records the weight in those moments. That is why the first weight registrations do not start in\n",
    "# the first time step and we add the initial weights manually.\n",
    "\n",
    "\n",
    "def plot_weight_time_course(ax, events, nrns, label, ylabel):\n",
    "    sender_label, target_label = label.split(\"_\")\n",
    "    nrns_senders = nrns[sender_label]\n",
    "    nrns_targets = nrns[target_label]\n",
    "\n",
    "    for sender in set(events_wr[\"senders\"]):\n",
    "        for target in set(events_wr[\"targets\"]):\n",
    "            if sender in nrns_senders and target in nrns_targets:\n",
    "                idc_syn = (events[\"senders\"] == sender) & (events[\"targets\"] == target)\n",
    "                if np.any(idc_syn):\n",
    "                    idc_syn_pre = (weights_pre_train[label][\"source\"] == sender) & (\n",
    "                        weights_pre_train[label][\"target\"] == target\n",
    "                    )\n",
    "                    times = np.concatenate([[0.0], events[\"times\"][idc_syn]])\n",
    "\n",
    "                    weights = np.concatenate(\n",
    "                        [np.array(weights_pre_train[label][\"weight\"])[idc_syn_pre], events[\"weights\"][idc_syn]]\n",
    "                    )\n",
    "                    ax.step(times, weights, c=colors[\"blue\"])\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_ylim(-0.6, 0.6)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, sharex=True, figsize=(3, 4))\n",
    "fig.suptitle(\"Weight time courses\")\n",
    "\n",
    "nrns = {\n",
    "    \"in\": nrns_in.tolist(),\n",
    "    \"rec\": nrns_rec.tolist(),\n",
    "    \"out\": nrns_out.tolist(),\n",
    "}\n",
    "\n",
    "plot_weight_time_course(axs[0], events_wr, nrns, \"in_rec\", r\"$W_\\text{in}$ (pA)\")\n",
    "plot_weight_time_course(axs[1], events_wr, nrns, \"rec_rec\", r\"$W_\\text{rec}$ (pA)\")\n",
    "plot_weight_time_course(axs[2], events_wr, nrns, \"rec_out\", r\"$W_\\text{out}$ (pA)\")\n",
    "\n",
    "axs[-1].set_xlabel(r\"$t$ (ms)\")\n",
    "axs[-1].set_xlim(0, duration[\"task\"])\n",
    "\n",
    "fig.align_ylabels()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb52b2f",
   "metadata": {
    "title": "###########################################################################################################"
   },
   "outputs": [],
   "source": [
    "# Plot weight matrices\n",
    "# ....................\n",
    "# If one is not interested in the time course of the weights, it is possible to read out only the initial and\n",
    "# final weights, which requires less computing time and memory than the weight recorder approach. Here, we plot\n",
    "# the corresponding weight matrices before and after the optimization.\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    \"cmap\", ((0.0, colors[\"blue\"]), (0.5, colors[\"white\"]), (1.0, colors[\"red\"]))\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, sharex=\"col\", sharey=\"row\")\n",
    "fig.suptitle(\"Weight matrices\")\n",
    "\n",
    "all_w_extrema = []\n",
    "\n",
    "for k in weights_pre_train.keys():\n",
    "    w_pre = weights_pre_train[k][\"weight\"]\n",
    "    w_post = weights_post_train[k][\"weight\"]\n",
    "    all_w_extrema.append([np.min(w_pre), np.max(w_pre), np.min(w_post), np.max(w_post)])\n",
    "\n",
    "args = {\"cmap\": cmap, \"vmin\": np.min(all_w_extrema), \"vmax\": np.max(all_w_extrema)}\n",
    "\n",
    "for i, weights in zip([0, 1], [weights_pre_train, weights_post_train]):\n",
    "    axs[0, i].pcolormesh(weights[\"in_rec\"][\"weight_matrix\"].T, **args)\n",
    "    axs[1, i].pcolormesh(weights[\"rec_rec\"][\"weight_matrix\"], **args)\n",
    "    cmesh = axs[2, i].pcolormesh(weights[\"rec_out\"][\"weight_matrix\"], **args)\n",
    "\n",
    "    axs[2, i].set_xlabel(\"recurrent\\nneurons\")\n",
    "\n",
    "axs[0, 0].set_ylabel(\"input\\nneurons\")\n",
    "axs[1, 0].set_ylabel(\"recurrent\\nneurons\")\n",
    "axs[2, 0].set_ylabel(\"readout\\nneurons\")\n",
    "fig.align_ylabels(axs[:, 0])\n",
    "\n",
    "axs[0, 0].text(0.5, 1.1, \"before training\", transform=axs[0, 0].transAxes, ha=\"center\")\n",
    "axs[0, 1].text(0.5, 1.1, \"after training\", transform=axs[0, 1].transAxes, ha=\"center\")\n",
    "\n",
    "axs[2, 0].yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "cbar = plt.colorbar(cmesh, cax=axs[1, 1].inset_axes([1.1, 0.2, 0.05, 0.8]), label=\"weight (pA)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "EBRAINS-experimental",
   "language": "python",
   "name": "ebrains-experimental"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}