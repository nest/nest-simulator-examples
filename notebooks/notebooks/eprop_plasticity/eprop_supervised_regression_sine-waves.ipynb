{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tutorial on learning to generate sine waves with e-prop\n\nTraining a regression model using supervised e-prop plasticity to generate sine waves\n\n## Description\n\nThis script demonstrates supervised learning of a regression task with a recurrent spiking neural network that\nis equipped with the eligibility propagation (e-prop) plasticity mechanism by Bellec et al. [1]_.\n\nThis type of learning is demonstrated at the proof-of-concept task in [1]_. We based this script on their\nTensorFlow script given in [2]_.\n\nIn this task, the network learns to generate an arbitrary N-dimensional temporal pattern. Here, the\nnetwork learns to reproduce with its overall spiking activity a one-dimensional, one-second-long target signal\nwhich is a superposition of four sine waves of different amplitudes, phases, and periods.\n\n<img src=\"file://../../../../pynest/examples/eprop_plasticity/eprop_supervised_regression_schematic_sine-waves.png\" width=\"70 %\" alt=\"See Figure 1 below.\" align=\"center\">\n\nLearning in the neural network model is achieved by optimizing the connection weights with e-prop plasticity.\nThis plasticity rule requires a specific network architecture depicted in Figure 1. The neural network model\nconsists of a recurrent network that receives frozen noise input from Poisson generators and projects onto one\nreadout neuron. The readout neuron compares the network signal $y$ with the teacher target signal\n$y*$, which it receives from a rate generator. In scenarios with multiple readout neurons, each individual\nreadout signal denoted as $y_k$ is compared with a corresponding target signal represented as\n$y_k^*$. The network's training error is assessed by employing a mean-squared error loss.\n\nDetails on the event-based NEST implementation of e-prop can be found in [3]_.\n\n## References\n\n.. [1] Bellec G, Scherr F, Subramoney F, Hajek E, Salaj D, Legenstein R, Maass W (2020). A solution to the\n       learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11:3625.\n       https://doi.org/10.1038/s41467-020-17236-y\n\n.. [2] https://github.com/IGITUGraz/eligibility_propagation/blob/master/Figure_3_and_S7_e_prop_tutorials/tutorial_pattern_generation.py\n\n.. [3] Korcsak-Gorzo A, Stapmanns J, Espinoza Valverde JA, Dahmen D, van Albada SJ, Bolten M, Diesmann M.\n       Event-based implementation of eligibility propagation (in preparation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries\nWe begin by importing all libraries required for the simulation, analysis, and visualization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport nest\nimport numpy as np\nfrom cycler import cycler\nfrom IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schematic of network architecture\nThis figure, identical to the one in the description, shows the required network architecture in the center,\nthe input and output of the pattern generation task above, and lists of the required NEST device, neuron, and\nsynapse models below. The connections that must be established are numbered 1 to 6.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    Image(filename=\"./eprop_supervised_regression_schematic_sine-waves.png\")\nexcept Exception:\n    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize random generator\nWe seed the numpy random generator, which will generate random initial weights as well as random input and\noutput.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rng_seed = 1  # numpy random seed\nnp.random.seed(rng_seed)  # fix numpy random seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define timing of task\nThe task's temporal structure is then defined, once as time steps and once as durations in milliseconds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_batch = 1  # batch size, 1 in reference [2]\nn_iter = 5  # number of iterations, 2000 in reference [2]\n\nsteps = {\n    \"sequence\": 1000,  # time steps of one full sequence\n}\n\nsteps[\"learning_window\"] = steps[\"sequence\"]  # time steps of window with non-zero learning signals\nsteps[\"task\"] = n_iter * n_batch * steps[\"sequence\"]  # time steps of task\n\nsteps.update(\n    {\n        \"offset_gen\": 1,  # offset since generator signals start from time step 1\n        \"delay_in_rec\": 1,  # connection delay between input and recurrent neurons\n        \"delay_rec_out\": 1,  # connection delay between recurrent and output neurons\n        \"delay_out_norm\": 1,  # connection delay between output neurons for normalization\n        \"extension_sim\": 1,  # extra time step to close right-open simulation time interval in Simulate()\n    }\n)\n\nsteps[\"delays\"] = steps[\"delay_in_rec\"] + steps[\"delay_rec_out\"] + steps[\"delay_out_norm\"]  # time steps of delays\n\nsteps[\"total_offset\"] = steps[\"offset_gen\"] + steps[\"delays\"]  # time steps of total offset\n\nsteps[\"sim\"] = steps[\"task\"] + steps[\"total_offset\"] + steps[\"extension_sim\"]  # time steps of simulation\n\nduration = {\"step\": 1.0}  # ms, temporal resolution of the simulation\n\nduration.update({key: value * duration[\"step\"] for key, value in steps.items()})  # ms, durations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set up simulation\nAs last step of the setup, we reset the NEST kernel to remove all existing NEST simulation settings and\nobjects and set some NEST kernel parameters, some of which are e-prop-related.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params_setup = {\n    \"eprop_learning_window\": duration[\"learning_window\"],\n    \"eprop_reset_neurons_on_update\": True,  # if True, reset dynamic variables at start of each update interval\n    \"eprop_update_interval\": duration[\"sequence\"],  # ms, time interval for updating the synaptic weights\n    \"print_time\": False,  # if True, print time progress bar during simulation, set False if run as code cell\n    \"resolution\": duration[\"step\"],\n    \"total_num_virtual_procs\": 1,  # number of virtual processes, set in case of distributed computing\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nest.ResetKernel()\nnest.set(**params_setup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create neurons\nWe proceed by creating a certain number of input, recurrent, and readout neurons and setting their parameters.\nAdditionally, we already create an input spike generator and an output target rate generator, which we will\nconfigure later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_in = 100  # number of input neurons\nn_rec = 100  # number of recurrent neurons\nn_out = 1  # number of readout neurons\n\nparams_nrn_rec = {\n    \"C_m\": 1.0,  # pF, membrane capacitance - takes effect only if neurons get current input (here not the case)\n    \"c_reg\": 300.0,  # firing rate regularization scaling\n    \"E_L\": 0.0,  # mV, leak / resting membrane potential\n    \"f_target\": 10.0,  # spikes/s, target firing rate for firing rate regularization\n    \"gamma\": 0.3,  # scaling of the pseudo derivative\n    \"I_e\": 0.0,  # pA, external current input\n    \"regular_spike_arrival\": False,  # If True, input spikes arrive at end of time step, if False at beginning\n    \"surrogate_gradient_function\": \"piecewise_linear\",  # surrogate gradient / pseudo-derivative function\n    \"t_ref\": 0.0,  # ms, duration of refractory period\n    \"tau_m\": 30.0,  # ms, membrane time constant\n    \"V_m\": 0.0,  # mV, initial value of the membrane voltage\n    \"V_th\": 0.03,  # mV, spike threshold membrane voltage\n}\n\nparams_nrn_out = {\n    \"C_m\": 1.0,\n    \"E_L\": 0.0,\n    \"I_e\": 0.0,\n    \"loss\": \"mean_squared_error\",  # loss function\n    \"regular_spike_arrival\": False,\n    \"tau_m\": 30.0,\n    \"V_m\": 0.0,\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Intermediate parrot neurons required between input spike generators and recurrent neurons,\n# since devices cannot establish plastic synapses for technical reasons\n\ngen_spk_in = nest.Create(\"spike_generator\", n_in)\nnrns_in = nest.Create(\"parrot_neuron\", n_in)\n\n# The suffix _bsshslm_2020 follows the NEST convention to indicate in the model name the paper\n# that introduced it by the first letter of the authors' last names and the publication year.\n\nnrns_rec = nest.Create(\"eprop_iaf_bsshslm_2020\", n_rec, params_nrn_rec)\nnrns_out = nest.Create(\"eprop_readout_bsshslm_2020\", n_out, params_nrn_out)\ngen_rate_target = nest.Create(\"step_rate_generator\", n_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create recorders\nWe also create recorders, which, while not required for the training, will allow us to track various dynamic\nvariables of the neurons, spikes, and changes in synaptic weights. To save computing time and memory, the\nrecorders, the recorded variables, neurons, and synapses can be limited to the ones relevant to the\nexperiment, and the recording interval can be increased (see the documentation on the specific recorders). By\ndefault, recordings are stored in memory but can also be written to file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_record = 1  # number of neurons to record dynamic variables from - this script requires n_record >= 1\nn_record_w = 3  # number of senders and targets to record weights from - this script requires n_record_w >=1\n\nif n_record == 0 or n_record_w == 0:\n    raise ValueError(\"n_record and n_record_w >= 1 required\")\n\nparams_mm_rec = {\n    \"interval\": duration[\"step\"],  # interval between two recorded time points\n    \"record_from\": [\"V_m\", \"surrogate_gradient\", \"learning_signal\"],  # dynamic variables to record\n    \"start\": duration[\"offset_gen\"] + duration[\"delay_in_rec\"],  # start time of recording\n    \"stop\": duration[\"offset_gen\"] + duration[\"delay_in_rec\"] + duration[\"task\"],  # stop time of recording\n}\n\nparams_mm_out = {\n    \"interval\": duration[\"step\"],\n    \"record_from\": [\"V_m\", \"readout_signal\", \"readout_signal_unnorm\", \"target_signal\", \"error_signal\"],\n    \"start\": duration[\"total_offset\"],\n    \"stop\": duration[\"total_offset\"] + duration[\"task\"],\n}\n\nparams_wr = {\n    \"senders\": nrns_in[:n_record_w] + nrns_rec[:n_record_w],  # limit senders to subsample weights to record\n    \"targets\": nrns_rec[:n_record_w] + nrns_out,  # limit targets to subsample weights to record from\n    \"start\": duration[\"total_offset\"],\n    \"stop\": duration[\"total_offset\"] + duration[\"task\"],\n}\n\nparams_sr = {\n    \"start\": duration[\"total_offset\"],\n    \"stop\": duration[\"total_offset\"] + duration[\"task\"],\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mm_rec = nest.Create(\"multimeter\", params_mm_rec)\nmm_out = nest.Create(\"multimeter\", params_mm_out)\nsr = nest.Create(\"spike_recorder\", params_sr)\nwr = nest.Create(\"weight_recorder\", params_wr)\n\nnrns_rec_record = nrns_rec[:n_record]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create connections\nNow, we define the connectivity and set up the synaptic parameters, with the synaptic weights drawn from\nnormal distributions. After these preparations, we establish the enumerated connections of the core network,\nas well as additional connections to the recorders.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params_conn_all_to_all = {\"rule\": \"all_to_all\", \"allow_autapses\": False}\nparams_conn_one_to_one = {\"rule\": \"one_to_one\"}\n\ndtype_weights = np.float32  # data type of weights - for reproducing TF results set to np.float32\nweights_in_rec = np.array(np.random.randn(n_in, n_rec).T / np.sqrt(n_in), dtype=dtype_weights)\nweights_rec_rec = np.array(np.random.randn(n_rec, n_rec).T / np.sqrt(n_rec), dtype=dtype_weights)\nnp.fill_diagonal(weights_rec_rec, 0.0)  # since no autapses set corresponding weights to zero\nweights_rec_out = np.array(np.random.randn(n_rec, n_out).T / np.sqrt(n_rec), dtype=dtype_weights)\nweights_out_rec = np.array(np.random.randn(n_rec, n_out) / np.sqrt(n_rec), dtype=dtype_weights)\n\nparams_common_syn_eprop = {\n    \"optimizer\": {\n        \"type\": \"gradient_descent\",  # algorithm to optimize the weights\n        \"batch_size\": n_batch,\n        \"eta\": 1e-4,  # learning rate\n        \"Wmin\": -100.0,  # pA, minimal limit of the synaptic weights\n        \"Wmax\": 100.0,  # pA, maximal limit of the synaptic weights\n    },\n    \"average_gradient\": False,  # if True, average the gradient over the learning window\n    \"weight_recorder\": wr,\n}\n\nparams_syn_base = {\n    \"synapse_model\": \"eprop_synapse_bsshslm_2020\",\n    \"delay\": duration[\"step\"],  # ms, dendritic delay\n    \"tau_m_readout\": params_nrn_out[\"tau_m\"],  # ms, for technical reasons pass readout neuron membrane time constant\n}\n\nparams_syn_in = params_syn_base.copy()\nparams_syn_in[\"weight\"] = weights_in_rec  # pA, initial values for the synaptic weights\n\nparams_syn_rec = params_syn_base.copy()\nparams_syn_rec[\"weight\"] = weights_rec_rec\n\nparams_syn_out = params_syn_base.copy()\nparams_syn_out[\"weight\"] = weights_rec_out\n\nparams_syn_feedback = {\n    \"synapse_model\": \"eprop_learning_signal_connection_bsshslm_2020\",\n    \"delay\": duration[\"step\"],\n    \"weight\": weights_out_rec,\n}\n\nparams_syn_rate_target = {\n    \"synapse_model\": \"rate_connection_delayed\",\n    \"delay\": duration[\"step\"],\n    \"receptor_type\": 2,  # receptor type over which readout neuron receives target signal\n}\n\nparams_syn_static = {\n    \"synapse_model\": \"static_synapse\",\n    \"delay\": duration[\"step\"],\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nest.SetDefaults(\"eprop_synapse_bsshslm_2020\", params_common_syn_eprop)\n\nnest.Connect(gen_spk_in, nrns_in, params_conn_one_to_one, params_syn_static)  # connection 1\nnest.Connect(nrns_in, nrns_rec, params_conn_all_to_all, params_syn_in)  # connection 2\nnest.Connect(nrns_rec, nrns_rec, params_conn_all_to_all, params_syn_rec)  # connection 3\nnest.Connect(nrns_rec, nrns_out, params_conn_all_to_all, params_syn_out)  # connection 4\nnest.Connect(nrns_out, nrns_rec, params_conn_all_to_all, params_syn_feedback)  # connection 5\nnest.Connect(gen_rate_target, nrns_out, params_conn_one_to_one, params_syn_rate_target)  # connection 6\n\nnest.Connect(nrns_in + nrns_rec, sr, params_conn_all_to_all, params_syn_static)\n\nnest.Connect(mm_rec, nrns_rec_record, params_conn_all_to_all, params_syn_static)\nnest.Connect(mm_out, nrns_out, params_conn_all_to_all, params_syn_static)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create input\nWe generate some frozen Poisson spike noise of a fixed rate that is repeated in each iteration and feed these\nspike times to the previously created input spike generator. The network will use these spike times as a\ntemporal backbone for encoding the target signal into its recurrent spiking activity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_spike_prob = 0.05  # spike probability of frozen input noise\ndtype_in_spks = np.float32  # data type of input spikes - for reproducing TF results set to np.float32\n\ninput_spike_bools = (np.random.rand(steps[\"sequence\"], n_in) < input_spike_prob).swapaxes(0, 1)\ninput_spike_bools[:, 0] = 0  # remove spikes in 0th time step of every sequence for technical reasons\n\nsequence_starts = np.arange(0.0, duration[\"task\"], duration[\"sequence\"]) + duration[\"offset_gen\"]\nparams_gen_spk_in = []\nfor input_spike_bool in input_spike_bools:\n    input_spike_times = np.arange(0.0, duration[\"sequence\"], duration[\"step\"])[input_spike_bool]\n    input_spike_times_all = [input_spike_times + start for start in sequence_starts]\n    params_gen_spk_in.append({\"spike_times\": np.hstack(input_spike_times_all).astype(dtype_in_spks)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nest.SetStatus(gen_spk_in, params_gen_spk_in)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create output\nThen, as a superposition of four sine waves with various durations, amplitudes, and phases, we construct a\none-second target signal. This signal, like the input, is repeated for all iterations and fed into the rate\ngenerator that was previously created.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_superimposed_sines(steps_sequence, periods):\n    n_sines = len(periods)\n\n    amplitudes = np.random.uniform(low=0.5, high=2.0, size=n_sines)\n    phases = np.random.uniform(low=0.0, high=2.0 * np.pi, size=n_sines)\n\n    sines = [\n        A * np.sin(np.linspace(phi, phi + 2.0 * np.pi * (steps_sequence // T), steps_sequence))\n        for A, phi, T in zip(amplitudes, phases, periods)\n    ]\n\n    superposition = sum(sines)\n    superposition -= superposition[0]\n    superposition /= max(np.abs(superposition).max(), 1e-6)\n    return superposition\n\n\ntarget_signal = generate_superimposed_sines(steps[\"sequence\"], [1000, 500, 333, 200])  # periods in steps\n\nparams_gen_rate_target = {\n    \"amplitude_times\": np.arange(0.0, duration[\"task\"], duration[\"step\"]) + duration[\"total_offset\"],\n    \"amplitude_values\": np.tile(target_signal, n_iter * n_batch),\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nest.SetStatus(gen_rate_target, params_gen_rate_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Force final update\nSynapses only get active, that is, the correct weight update calculated and applied, when they transmit a\nspike. To still be able to read out the correct weights at the end of the simulation, we force spiking of the\npresynaptic neuron and thus an update of all synapses, including those that have not transmitted a spike in\nthe last update interval, by sending a strong spike to all neurons that form the presynaptic side of an eprop\nsynapse. This step is required purely for technical reasons.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gen_spk_final_update = nest.Create(\"spike_generator\", 1, {\"spike_times\": [duration[\"task\"] + duration[\"delays\"]]})\n\nnest.Connect(gen_spk_final_update, nrns_in + nrns_rec, \"all_to_all\", {\"weight\": 1000.0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read out pre-training weights\nBefore we begin training, we read out the initial weight matrices so that we can eventually compare them to\nthe optimized weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_weights(pop_pre, pop_post):\n    conns = nest.GetConnections(pop_pre, pop_post).get([\"source\", \"target\", \"weight\"])\n    conns[\"senders\"] = np.array(conns[\"source\"]) - np.min(conns[\"source\"])\n    conns[\"targets\"] = np.array(conns[\"target\"]) - np.min(conns[\"target\"])\n\n    conns[\"weight_matrix\"] = np.zeros((len(pop_post), len(pop_pre)))\n    conns[\"weight_matrix\"][conns[\"targets\"], conns[\"senders\"]] = conns[\"weight\"]\n    return conns\n\n\nweights_pre_train = {\n    \"in_rec\": get_weights(nrns_in, nrns_rec),\n    \"rec_rec\": get_weights(nrns_rec, nrns_rec),\n    \"rec_out\": get_weights(nrns_rec, nrns_out),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate\nWe train the network by simulating for a set simulation time, determined by the number of iterations and the\nbatch size and the length of one sequence.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nest.Simulate(duration[\"sim\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read out post-training weights\nAfter the training, we can read out the optimized final weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights_post_train = {\n    \"in_rec\": get_weights(nrns_in, nrns_rec),\n    \"rec_rec\": get_weights(nrns_rec, nrns_rec),\n    \"rec_out\": get_weights(nrns_rec, nrns_out),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read out recorders\nWe can also retrieve the recorded history of the dynamic variables and weights, as well as detected spikes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events_mm_rec = mm_rec.get(\"events\")\nevents_mm_out = mm_out.get(\"events\")\nevents_sr = sr.get(\"events\")\nevents_wr = wr.get(\"events\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate training error\nWe evaluate the network's training error by calculating a loss - in this case, the mean squared error between\nthe integrated recurrent network activity and the target rate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "readout_signal = events_mm_out[\"readout_signal\"]\ntarget_signal = events_mm_out[\"target_signal\"]\n\nerror = (readout_signal - target_signal) ** 2\nloss = 0.5 * np.add.reduceat(error, np.arange(0, steps[\"task\"], steps[\"sequence\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot results\nThen, we plot a series of plots.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "do_plotting = True  # if True, plot the results\n\nif not do_plotting:\n    exit()\n\ncolors = {\n    \"blue\": \"#2854c5ff\",\n    \"red\": \"#e04b40ff\",\n    \"white\": \"#ffffffff\",\n}\n\nplt.rcParams.update(\n    {\n        \"font.sans-serif\": \"Arial\",\n        \"axes.spines.right\": False,\n        \"axes.spines.top\": False,\n        \"axes.prop_cycle\": cycler(color=[colors[\"blue\"], colors[\"red\"]]),\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot training error\nWe begin with a plot visualizing the training error of the network: the loss plotted against the iterations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n\nax.plot(range(1, n_iter + 1), loss)\nax.set_ylabel(r\"$E = \\frac{1}{2} \\sum_{t,k} \\left( y_k^t -y_k^{*,t}\\right)^2$\")\nax.set_xlabel(\"training iteration\")\nax.set_xlim(1, n_iter)\nax.xaxis.get_major_locator().set_params(integer=True)\n\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot spikes and dynamic variables\nThis plotting routine shows how to plot all of the recorded dynamic variables and spikes across time. We take\none snapshot in the first iteration and one snapshot at the end.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_recordable(ax, events, recordable, ylabel, xlims):\n    for sender in set(events[\"senders\"]):\n        idc_sender = events[\"senders\"] == sender\n        idc_times = (events[\"times\"][idc_sender] > xlims[0]) & (events[\"times\"][idc_sender] < xlims[1])\n        ax.plot(events[\"times\"][idc_sender][idc_times], events[recordable][idc_sender][idc_times], lw=0.5)\n    ax.set_ylabel(ylabel)\n    margin = np.abs(np.max(events[recordable]) - np.min(events[recordable])) * 0.1\n    ax.set_ylim(np.min(events[recordable]) - margin, np.max(events[recordable]) + margin)\n\n\ndef plot_spikes(ax, events, nrns, ylabel, xlims):\n    idc_times = (events[\"times\"] > xlims[0]) & (events[\"times\"] < xlims[1])\n    idc_sender = np.isin(events[\"senders\"][idc_times], nrns.tolist())\n    senders_subset = events[\"senders\"][idc_times][idc_sender]\n    times_subset = events[\"times\"][idc_times][idc_sender]\n\n    ax.scatter(times_subset, senders_subset, s=0.1)\n    ax.set_ylabel(ylabel)\n    margin = np.abs(np.max(senders_subset) - np.min(senders_subset)) * 0.1\n    ax.set_ylim(np.min(senders_subset) - margin, np.max(senders_subset) + margin)\n\n\nfor xlims in [(0, steps[\"sequence\"]), (steps[\"task\"] - steps[\"sequence\"], steps[\"task\"])]:\n    fig, axs = plt.subplots(9, 1, sharex=True, figsize=(6, 8), gridspec_kw={\"hspace\": 0.4, \"left\": 0.2})\n\n    plot_spikes(axs[0], events_sr, nrns_in, r\"$z_i$\" + \"\\n\", xlims)\n    plot_spikes(axs[1], events_sr, nrns_rec, r\"$z_j$\" + \"\\n\", xlims)\n\n    plot_recordable(axs[2], events_mm_rec, \"V_m\", r\"$v_j$\" + \"\\n(mV)\", xlims)\n    plot_recordable(axs[3], events_mm_rec, \"surrogate_gradient\", r\"$\\psi_j$\" + \"\\n\", xlims)\n    plot_recordable(axs[4], events_mm_rec, \"learning_signal\", r\"$L_j$\" + \"\\n(pA)\", xlims)\n\n    plot_recordable(axs[5], events_mm_out, \"V_m\", r\"$v_k$\" + \"\\n(mV)\", xlims)\n    plot_recordable(axs[6], events_mm_out, \"target_signal\", r\"$y^*_k$\" + \"\\n\", xlims)\n    plot_recordable(axs[7], events_mm_out, \"readout_signal\", r\"$y_k$\" + \"\\n\", xlims)\n    plot_recordable(axs[8], events_mm_out, \"error_signal\", r\"$y_k-y^*_k$\" + \"\\n\", xlims)\n\n    axs[-1].set_xlabel(r\"$t$ (ms)\")\n    axs[-1].set_xlim(*xlims)\n\n    fig.align_ylabels()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot weight time courses\nSimilarly, we can plot the weight histories. Note that the weight recorder, attached to the synapses, works\ndifferently than the other recorders. Since synapses only get activated when they transmit a spike, the weight\nrecorder only records the weight in those moments. That is why the first weight registrations do not start in\nthe first time step and we add the initial weights manually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_weight_time_course(ax, events, nrns_senders, nrns_targets, label, ylabel):\n    for sender in nrns_senders.tolist():\n        for target in nrns_targets.tolist():\n            idc_syn = (events[\"senders\"] == sender) & (events[\"targets\"] == target)\n            idc_syn_pre = (weights_pre_train[label][\"source\"] == sender) & (\n                weights_pre_train[label][\"target\"] == target\n            )\n\n            times = [0.0] + events[\"times\"][idc_syn].tolist()\n            weights = [weights_pre_train[label][\"weight\"][idc_syn_pre]] + events[\"weights\"][idc_syn].tolist()\n\n            ax.step(times, weights, c=colors[\"blue\"])\n        ax.set_ylabel(ylabel)\n        ax.set_ylim(-0.6, 0.6)\n\n\nfig, axs = plt.subplots(3, 1, sharex=True, figsize=(3, 4))\n\nplot_weight_time_course(axs[0], events_wr, nrns_in[:n_record_w], nrns_rec[:n_record_w], \"in_rec\", r\"$W_\\text{in}$ (pA)\")\nplot_weight_time_course(\n    axs[1], events_wr, nrns_rec[:n_record_w], nrns_rec[:n_record_w], \"rec_rec\", r\"$W_\\text{rec}$ (pA)\"\n)\nplot_weight_time_course(axs[2], events_wr, nrns_rec[:n_record_w], nrns_out, \"rec_out\", r\"$W_\\text{out}$ (pA)\")\n\naxs[-1].set_xlabel(r\"$t$ (ms)\")\naxs[-1].set_xlim(0, steps[\"task\"])\n\nfig.align_ylabels()\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot weight matrices\nIf one is not interested in the time course of the weights, it is possible to read out only the initial and\nfinal weights, which requires less computing time and memory than the weight recorder approach. Here, we plot\nthe corresponding weight matrices before and after the optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n    \"cmap\", ((0.0, colors[\"blue\"]), (0.5, colors[\"white\"]), (1.0, colors[\"red\"]))\n)\n\nfig, axs = plt.subplots(3, 2, sharex=\"col\", sharey=\"row\")\n\nall_w_extrema = []\n\nfor k in weights_pre_train.keys():\n    w_pre = weights_pre_train[k][\"weight\"]\n    w_post = weights_post_train[k][\"weight\"]\n    all_w_extrema.append([np.min(w_pre), np.max(w_pre), np.min(w_post), np.max(w_post)])\n\nargs = {\"cmap\": cmap, \"vmin\": np.min(all_w_extrema), \"vmax\": np.max(all_w_extrema)}\n\nfor i, weights in zip([0, 1], [weights_pre_train, weights_post_train]):\n    axs[0, i].pcolormesh(weights[\"in_rec\"][\"weight_matrix\"].T, **args)\n    axs[1, i].pcolormesh(weights[\"rec_rec\"][\"weight_matrix\"], **args)\n    cmesh = axs[2, i].pcolormesh(weights[\"rec_out\"][\"weight_matrix\"], **args)\n\n    axs[2, i].set_xlabel(\"recurrent\\nneurons\")\n\naxs[0, 0].set_ylabel(\"input\\nneurons\")\naxs[1, 0].set_ylabel(\"recurrent\\nneurons\")\naxs[2, 0].set_ylabel(\"readout\\nneurons\")\nfig.align_ylabels(axs[:, 0])\n\naxs[0, 0].text(0.5, 1.1, \"pre-training\", transform=axs[0, 0].transAxes, ha=\"center\")\naxs[0, 1].text(0.5, 1.1, \"post-training\", transform=axs[0, 1].transAxes, ha=\"center\")\n\naxs[2, 0].yaxis.get_major_locator().set_params(integer=True)\n\ncbar = plt.colorbar(cmesh, cax=axs[1, 1].inset_axes([1.1, 0.2, 0.05, 0.8]), label=\"weight (pA)\")\n\nfig.tight_layout()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EBRAINS-23.09",
      "language": "python",
      "name": "ebrains-23.09"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}